
Qwen3-Coder-480B-N


Project Sentinel - Conversation Index
This index provides a topic-based roadmap through the complete conversation history of the "Sentinel" software development project. It is designed for a human developer seeking to understand the project's evolution, architecture, and current state.

Project Charter & Philosophy
High-Level Summary: The foundational vision for Project Sentinel, codenamed "The Sentinel Portfolio Manager," was established. It is designed as a semi-automated, AI-driven swing trading system that prioritizes safety, human oversight, and robustness over raw automation. The core philosophy is "AI-Maximalist, Code-Minimalist."

Core Rules Defined: The system's operation is governed by strict, non-negotiable rules: Local-Only Operation (run on a monitored machine), Human-in-the-Loop Approval (via SMS), Default to Safety (halt if unsure), Portfolio-Centric Logic (rebalance a 90% invested portfolio), and an Autonomous Fail-Safe ("Dead Man's Switch").
AI Integration Strategy: The plan explicitly integrates AI for complex tasks like market analysis and news summarization, while using simple, auditable code for structured operations like data handling and trade execution.
System Workflow Outlined: A clear five-stage end-of-day workflow was defined: System Initialization & State Review, Candidate Universe Generation, Data Dossier Aggregation, AI-Powered Analysis & Logging, and Database, Approval & Execution.
APIs Identified: The core technical dependencies were established: Alpaca (trading/pricing), yfinance (fundamentals), Perplexity (general news/context), OpenAI (analysis), and Twilio (SMS).
Initial Prototype (v1-v2)
High-Level Summary: The first functional prototypes were developed, moving from a static, hardcoded script to a dynamic one connected to live market APIs.

Static Prototype (v1.0): A basic main_script.py was created using fixed, dummy data to test the core portfolio rebalancing logic (the calculate_trade_plan function based on 90% invested and 10% max position rules).
Live Data Integration (v2.0): The prototype was upgraded to connect to the Alpaca API for live account status and position data, replacing the static placeholders. This introduced config.py for secrets and .gitignore for security.
AI-Powered Analysis (v3.0): OpenAI's GPT-4 was integrated to perform stock analysis. The script now generates dynamic "dossiers" for each stock, combining fundamentals (yfinance), historical data (Alpaca), and news (Alpaca News API), and sends them to the AI for a "Buy"/"Sell"/"Hold" decision with a conviction score.
News Context Enhancement (v4.0): Recognizing the AI needed broader market context, a two-stage news gathering process was introduced. Perplexity's /search endpoint fetches general market news, which is then summarized by OpenAI and passed to the stock-specific AI analysis.
Alpaca API Integration (Account Data)
High-Level Summary: The system established a reliable connection to the Alpaca API for retrieving account information, market data, and executing trades in a paper trading environment.

Authentication & Account Review: The get_alpaca_api() and get_account_info() functions were implemented to securely authenticate using keys from config.py and fetch real-time portfolio value and open positions.
Historical Data Fetching: The aggregate_data_dossiers() function uses api.get_bars() to pull one year of daily price/volume data for the candidate universe.
Stock-Specific News: The get_stock_specific_news_headlines() function fetches recent headlines for a given stock via api.get_news().
Trade Execution (Planned): The architecture includes a plan for api.submit_order() to place live trades once the approval system is in place.
AI Integration (OpenAI & Prompt Engineering)
High-Level Summary: OpenAI's GPT-4 Turbo model was integrated for AI-powered stock analysis and market context summarization, with a strong emphasis on prompt engineering for reliable, structured output.

Strategic Analysis Prompt: A detailed, structured prompt was engineered for get_ai_analysis(). It instructs the AI to act as a quantitative analyst, ingest the stock dossier, and return a decision in a strict JSON format: {"symbol", "decision", "conviction_score", "rationale"}.
Market Context Summarization Prompt: A prompt was created for summarize_market_context_with_openai() to take raw Perplexity search results and synthesize them into a clean, coherent market summary report.
Response Format Enforcement: The response_format={"type": "json_object"} parameter was used consistently to guide the AI towards structured output, reducing parsing errors.
Error Handling & Validation: Basic error handling and validation were added to catch API failures and ensure the AI response contains the required JSON keys.
State Management (Database Implementation)
High-Level Summary: A local SQLite database (sentinel.db) was implemented to provide robust, persistent state management, logging, and a "fail-safe" mechanism.

Database Setup Script: A dedicated database_setup.py script was created to initialize the SQLite database and create the necessary tables (decisions, trades) using standard SQL.
Daily Run Check: The check_if_trades_executed_today() function queries the trades table to see if trades were already submitted for the current day, preventing duplicate runs.
Decision Logging: The log_decision_to_db() function stores every AI-generated decision (symbol, decision, conviction, rationale, etc.) in the decisions table.
Trade Logging: Placeholder logic was added (and later refined) to log approved and executed trades into the trades table, providing an audit trail.
Persistence: This system ensures the bot's "memory" survives reboots and allows for future performance analysis.
Intelligence Enhancement (Perplexity API)
High-Level Summary: The Perplexity API was integrated to enhance the system's market intelligence by providing real-time, web-search-derived context.

General Market Context: The get_raw_search_results_from_perplexity() function calls Perplexity's /search endpoint with a query like "Top 15-20 most significant, market-moving financial news stories last 24 hours."
API Key Management: Perplexity API credentials were added to config.py and loaded via python-dotenv.
Search Result Utilization: The raw search results from Perplexity are fed into the OpenAI summarization function, providing the AI with a high-level understanding of the current market environment.
Endpoint Correction: An initial error using the /chat/completions endpoint was identified and corrected to use the /search endpoint, which is compatible with the user's API key.
Live Trading Logic & Safety Features
High-Level Summary: Core trading logic and multiple safety features were implemented or planned to govern the bot's interaction with a live brokerage account.

Capital Allocation Engine: The calculate_trade_plan() function implements the Charter's "90% invested capital rule" and "risk-per-trade limits" by distributing capital based on AI conviction scores and capping individual positions at 10% portfolio value.
Rebalancing Logic: The system analyzes its current portfolio (from Alpaca) against the AI's target portfolio and generates precise BUY/SELL orders to bridge the gap.
Master Safety Switch (LIVE_TRADING): A boolean flag in config.py (LIVE_TRADING = False/True) was introduced to cleanly separate "Safe Mode" (simulated trades) from "Live Mode" (actual API calls).
Approval Workflow: An interactive approval process was implemented where the proposed trade plan is presented to the user. Execution only proceeds upon receiving an explicit "APPROVE" input via the console (SMS approval planned for later).
API Compliance: Order values are rounded to 2 decimal places (round(value, 2)) to meet Alpaca API requirements, fixing a live trading bug.
Bug Fixes (List specific bugs)
High-Level Summary: Several critical bugs were identified, diagnosed, and resolved through iterative testing and debugging, significantly hardening the system.

"Not Null Constraint Failed" (v5.3): A bug where the AI sometimes returned None or malformed JSON caused database insertion failures. Fixed by adding defensive checks in get_ai_analysis() to validate the AI's response structure before attempting to log it.
"Float Division by Zero" in Performance Report (v5.3): The YTD P/L calculation failed for new accounts with $0 starting value. Fixed by adding a check if ytd_start_value > 0: before performing the division.
"lxml/html5lib Dependency" Errors (v5.5): pandas.read_html failed due to missing parsing libraries. Fixed by running pip install lxml html5lib to provide the necessary dependencies.
Incorrect Perplexity Model Name (v5.5): Using a deprecated model name (llama-3-sonar-large-32k-online) caused API errors. Fixed by switching to a valid model (pplx-70b-online) for the /chat/completions endpoint.
API Key Scope / Model Access Issue (v6.1): Despite a valid key, calls to Perplexity's Chat Completions API failed. Diagnosed as the key being provisioned for the Search API only. Fixed by modifying the Perplexity integration to correctly call the /search endpoint with a query payload.
Too Many Decimal Places in Trade Orders (v6.1): Alpaca API rejected orders due to dollar amounts having more than 2 decimal places. Fixed by adding round(value, 2) to calculated notional trade values.
GitHub Push Protection Triggered (v6.1): Accidentally committing backup files (.bak) containing API keys triggered GitHub's security. Fixed by resetting the commit (git reset --soft HEAD~1), deleting sensitive files, and updating .gitignore to exclude backups and archives.
Security (Secrets Management, .gitignore)
High-Level Summary: Rigorous security practices were implemented from the outset to protect sensitive API credentials and ensure they are never committed to version control.

config.py for Secrets: All API keys (Alpaca, OpenAI, Perplexity, Twilio) are stored in a dedicated config.py file, separate from the main application logic.
.gitignore File: A .gitignore file was created and meticulously maintained to explicitly prevent Git from tracking config.py, the database file (sentinel.db), backup files (*.bak), and archive folders (_archive/).
Environment Variables via python-dotenv: The python-dotenv library is used to load secrets from config.py into the script's environment, keeping them out of the main code.
API Key Scope Awareness: The debugging process revealed the importance of understanding API key permissions (e.g., Search vs. Chat access), leading to adapting the code to the correct endpoint.
Documentation & Recovery Plan
High-Level Summary: Comprehensive documentation and a robust recovery plan were created to ensure the project's long-term maintainability and the ability to resume development after breaks.

README.md Creation: A detailed README.md file was written, serving as a high-level technical and logical guide to the system's architecture, workflow, and core principles.
DESIGN_LOG.md Creation: A comprehensive DESIGN_LOG.md was created to chronicle key architectural decisions, bug fixes, and strategic pivots, providing a condensed history of the project's evolution.
START_HERE_WHEN_YOU_RETURN.txt Time Capsule: A critical "time capsule" file was created with step-by-step instructions for anyone (including the developer's future self) to quickly get the project running again and bring a new AI assistant up to speed.
Conversation History Archival: The user was instructed to save the full conversation history locally for future reference by the "Context Manager."
GitHub Integration: All new files (README.md, DESIGN_LOG.md, START_HERE...txt) were committed and pushed to the project's GitHub repository, ensuring an off-site, version-controlled backup.

